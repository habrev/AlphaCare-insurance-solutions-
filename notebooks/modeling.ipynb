{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5405/1201793596.py:5: DtypeWarning: Columns (32,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('../assets/data/MachineLearningRating_v3.txt', delimiter= '|')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data per column:\n",
      "UnderwrittenCoverID               0\n",
      "PolicyID                          0\n",
      "TransactionMonth                  0\n",
      "IsVATRegistered                   0\n",
      "Citizenship                       0\n",
      "LegalType                         0\n",
      "Title                             0\n",
      "Language                          0\n",
      "Bank                         145961\n",
      "AccountType                   40232\n",
      "MaritalStatus                  8259\n",
      "Gender                         9536\n",
      "Country                           0\n",
      "Province                          0\n",
      "PostalCode                        0\n",
      "MainCrestaZone                    0\n",
      "SubCrestaZone                     0\n",
      "ItemType                          0\n",
      "mmcode                          552\n",
      "VehicleType                     552\n",
      "RegistrationYear                  0\n",
      "make                            552\n",
      "Model                           552\n",
      "Cylinders                       552\n",
      "cubiccapacity                   552\n",
      "kilowatts                       552\n",
      "bodytype                        552\n",
      "NumberOfDoors                   552\n",
      "VehicleIntroDate                552\n",
      "CustomValueEstimate          779642\n",
      "AlarmImmobiliser                  0\n",
      "TrackingDevice                    0\n",
      "CapitalOutstanding                2\n",
      "NewVehicle                   153295\n",
      "WrittenOff                   641901\n",
      "Rebuilt                      641901\n",
      "Converted                    641901\n",
      "CrossBorder                  999400\n",
      "NumberOfVehiclesInFleet     1000098\n",
      "SumInsured                        0\n",
      "TermFrequency                     0\n",
      "CalculatedPremiumPerTerm          0\n",
      "ExcessSelected                    0\n",
      "CoverCategory                     0\n",
      "CoverType                         0\n",
      "CoverGroup                        0\n",
      "Section                           0\n",
      "Product                           0\n",
      "StatutoryClass                    0\n",
      "StatutoryRiskType                 0\n",
      "TotalPremium                      0\n",
      "TotalClaims                       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load your data\n",
    "data = pd.read_csv('../assets/data/MachineLearningRating_v3.txt', delimiter= '|')\n",
    "\n",
    "# Check for missing data\n",
    "missing_data = data.isnull().sum()\n",
    "print(f\"Missing data per column:\\n{missing_data}\")\n",
    "\n",
    "# Define a threshold for removing columns with too many missing values (e.g., more than 50%)\n",
    "threshold = 0.5\n",
    "columns_to_remove = missing_data[missing_data / len(data) > threshold].index\n",
    "data_cleaned = data.drop(columns=columns_to_remove)\n",
    "\n",
    "# For columns with less missing data, handle them differently:\n",
    "# Impute numerical columns with the mean\n",
    "numerical_columns = data_cleaned.select_dtypes(include=['float64', 'int64']).columns\n",
    "numerical_imputer = SimpleImputer(strategy='mean')\n",
    "data_cleaned[numerical_columns] = numerical_imputer.fit_transform(data_cleaned[numerical_columns])\n",
    "\n",
    "# Impute categorical columns with the most frequent value (mode)\n",
    "categorical_columns = data_cleaned.select_dtypes(include=['object']).columns\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "data_cleaned[categorical_columns] = categorical_imputer.fit_transform(data_cleaned[categorical_columns])\n",
    "\n",
    "# Optional: Save the cleaned data\n",
    "data_cleaned.to_csv('../assets/data/cleaned_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5122/1477367607.py:3: DtypeWarning: Columns (32,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('../assets/data/MachineLearningRating_v3.txt', delimiter= '|')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ClaimFrequency  LossRatio  ClaimsToPremiumDiff\n",
      "0                   0.0        0.0            21.929825\n",
      "1                   0.0        0.0            21.929825\n",
      "2                   0.0        0.0             0.000000\n",
      "3                   0.0        0.0           512.848070\n",
      "4                   0.0        0.0             0.000000\n",
      "...                 ...        ...                  ...\n",
      "1000093             0.0        0.0           347.235175\n",
      "1000094             0.0        0.0           347.235175\n",
      "1000095             0.0        0.0           347.235175\n",
      "1000096             0.0        0.0             2.315000\n",
      "1000097             0.0        0.0             2.315000\n",
      "\n",
      "[1000098 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load your data\n",
    "data = pd.read_csv('../assets/data/MachineLearningRating_v3.txt', delimiter= '|')\n",
    "# Create new features\n",
    "data['ClaimFrequency'] = data['TotalClaims'] / (data['TotalPremium'] + 1)  # Added 1 to avoid division by zero\n",
    "data['LossRatio'] = data['TotalClaims'] / (data['TotalPremium'] + 1)  # Added 1 for safety\n",
    "data['ClaimsToPremiumDiff'] = data['TotalPremium'] - data['TotalClaims']\n",
    "# Display the new features\n",
    "print(data[['ClaimFrequency', 'LossRatio', 'ClaimsToPremiumDiff']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5996/4056854883.py:9: DtypeWarning: Columns (32,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(txt_file_path, delimiter='|')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File has been converted and saved as: ../assets/data/MachineLearningRating_v3.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "txt_file_path = \"../assets/data/MachineLearningRating_v3.txt\"  \n",
    "csv_file_path = \"../assets/data/MachineLearningRating_v3.csv\"  \n",
    "\n",
    "df = pd.read_csv(txt_file_path, delimiter='|')\n",
    "\n",
    "# Save as a .csv file\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"File has been converted and saved as: {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding categorical column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6813/3999043657.py:4: DtypeWarning: Columns (32,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../assets/data/MachineLearningRating_v3.txt', delimiter= '|')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset:\n",
      "   UnderwrittenCoverID  PolicyID     TransactionMonth  IsVATRegistered  \\\n",
      "0               145249     12827  2015-03-01 00:00:00             True   \n",
      "1               145249     12827  2015-05-01 00:00:00             True   \n",
      "2               145249     12827  2015-07-01 00:00:00             True   \n",
      "3               145255     12827  2015-05-01 00:00:00             True   \n",
      "4               145255     12827  2015-07-01 00:00:00             True   \n",
      "\n",
      "  Citizenship          LegalType Title Language                 Bank  \\\n",
      "0              Close Corporation    Mr  English  First National Bank   \n",
      "1              Close Corporation    Mr  English  First National Bank   \n",
      "2              Close Corporation    Mr  English  First National Bank   \n",
      "3              Close Corporation    Mr  English  First National Bank   \n",
      "4              Close Corporation    Mr  English  First National Bank   \n",
      "\n",
      "       AccountType  ...                    ExcessSelected CoverCategory  \\\n",
      "0  Current account  ...             Mobility - Windscreen    Windscreen   \n",
      "1  Current account  ...             Mobility - Windscreen    Windscreen   \n",
      "2  Current account  ...             Mobility - Windscreen    Windscreen   \n",
      "3  Current account  ...  Mobility - Metered Taxis - R2000    Own damage   \n",
      "4  Current account  ...  Mobility - Metered Taxis - R2000    Own damage   \n",
      "\n",
      "    CoverType            CoverGroup              Section  \\\n",
      "0  Windscreen  Comprehensive - Taxi  Motor Comprehensive   \n",
      "1  Windscreen  Comprehensive - Taxi  Motor Comprehensive   \n",
      "2  Windscreen  Comprehensive - Taxi  Motor Comprehensive   \n",
      "3  Own Damage  Comprehensive - Taxi  Motor Comprehensive   \n",
      "4  Own Damage  Comprehensive - Taxi  Motor Comprehensive   \n",
      "\n",
      "                           Product StatutoryClass StatutoryRiskType  \\\n",
      "0  Mobility Metered Taxis: Monthly     Commercial     IFRS Constant   \n",
      "1  Mobility Metered Taxis: Monthly     Commercial     IFRS Constant   \n",
      "2  Mobility Metered Taxis: Monthly     Commercial     IFRS Constant   \n",
      "3  Mobility Metered Taxis: Monthly     Commercial     IFRS Constant   \n",
      "4  Mobility Metered Taxis: Monthly     Commercial     IFRS Constant   \n",
      "\n",
      "   TotalPremium TotalClaims  \n",
      "0     21.929825         0.0  \n",
      "1     21.929825         0.0  \n",
      "2      0.000000         0.0  \n",
      "3    512.848070         0.0  \n",
      "4      0.000000         0.0  \n",
      "\n",
      "[5 rows x 52 columns]\n",
      "\n",
      "Categorical Columns: Index(['TransactionMonth', 'Citizenship', 'LegalType', 'Title', 'Language',\n",
      "       'Bank', 'AccountType', 'MaritalStatus', 'Gender', 'Country', 'Province',\n",
      "       'MainCrestaZone', 'SubCrestaZone', 'ItemType', 'VehicleType', 'make',\n",
      "       'Model', 'bodytype', 'VehicleIntroDate', 'AlarmImmobiliser',\n",
      "       'TrackingDevice', 'CapitalOutstanding', 'NewVehicle', 'WrittenOff',\n",
      "       'Rebuilt', 'Converted', 'CrossBorder', 'TermFrequency',\n",
      "       'ExcessSelected', 'CoverCategory', 'CoverType', 'CoverGroup', 'Section',\n",
      "       'Product', 'StatutoryClass', 'StatutoryRiskType'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv('../assets/data/MachineLearningRating_v3.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"Original Dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Step 2: Identify categorical columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "print(\"\\nCategorical Columns:\", categorical_cols)\n",
    "\n",
    "# Step 3: Apply One-Hot Encoding\n",
    "df_one_hot = pd.get_dummies(df, columns=categorical_cols)\n",
    "\n",
    "# Save the one-hot encoded dataset to a new file (optional)\n",
    "df_one_hot.to_csv(\"../assets/data/dataset_one_hot_encoded.csv\", index=False)\n",
    "\n",
    "print(\"\\nOne-Hot Encoded Dataset:\")\n",
    "print(df_one_hot.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Apply Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Create a copy of the dataset for label encoding\n",
    "df_label_encoded = df.copy()\n",
    "\n",
    "# Apply label encoding to each categorical column\n",
    "for col in categorical_cols:\n",
    "    df_label_encoded[col + \"_encoded\"] = label_encoder.fit_transform(df_label_encoded[col])\n",
    "\n",
    "# Save the label encoded dataset to a new file (optional)\n",
    "df_label_encoded.to_csv(\"../assets/data/dataset_label_encoded.csv\", index=False)\n",
    "\n",
    "print(\"\\nLabel Encoded Dataset:\")\n",
    "print(df_label_encoded.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12600/3847089309.py:6: DtypeWarning: Columns (32,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (800078, 52)\n",
      "Test data shape: (200020, 52)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"../assets/data/MachineLearningRating_v3.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shapes of the resulting datasets\n",
    "print(\"Training data shape:\", train_data.shape)\n",
    "print(\"Test data shape:\", test_data.shape)\n",
    "\n",
    "# Save the datasets separately\n",
    "train_data.to_csv(\"../assets/data/train_data.csv\", index=False)\n",
    "test_data.to_csv(\"../assets/data/test_data.csv\", index=False)\n",
    "\n",
    "print(\"Training and test data saved as 'train_data.csv' and 'test_data.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining rows after dropping NaN values: 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m y \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msynthetic_target\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Train-Test Split\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Linear Regression\u001b[39;00m\n\u001b[1;32m     35\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/Desktop/KAIM/week 3/tak-4/AlphaCare-insurance-solutions-/.venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/KAIM/week 3/tak-4/AlphaCare-insurance-solutions-/.venv/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2851\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2848\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[1;32m   2850\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m-> 2851\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[1;32m   2853\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m   2856\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/KAIM/week 3/tak-4/AlphaCare-insurance-solutions-/.venv/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2481\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2478\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2481\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2482\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2483\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2484\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[1;32m   2485\u001b[0m     )\n\u001b[1;32m   2487\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import time\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"../assets/data/train_data.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Use only 10% of the data to reduce resource usage\n",
    "data = data.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Ensure only numeric columns are included for generating the synthetic target\n",
    "numeric_data = data.select_dtypes(include=['number'])\n",
    "\n",
    "# Generate a synthetic target column (Mean of numeric columns for each row)\n",
    "data['synthetic_target'] = numeric_data.mean(axis=1)\n",
    "\n",
    "# Drop rows with NaN values (if any were created during this process)\n",
    "data = data.dropna()\n",
    "\n",
    "\n",
    "# Check how many rows remain after dropping NaN values\n",
    "print(f\"Remaining rows after dropping NaN values: {len(data)}\")\n",
    "\n",
    "# Features and target\n",
    "X = data.drop(columns=['synthetic_target'])\n",
    "y = data['synthetic_target']\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Linear Regression\n",
    "start_time = time.time()\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_preds = lr_model.predict(X_test)\n",
    "print(\"Linear Regression:\")\n",
    "print(\"MSE:\", mean_squared_error(y_test, lr_preds))\n",
    "print(\"R2 Score:\", r2_score(y_test, lr_preds))\n",
    "print(f\"Elapsed Time: {time.time() - start_time:.2f} seconds\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      3\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)  \u001b[38;5;66;03m# Limited depth and estimators\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m rf_model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[1;32m      5\u001b[0m rf_preds \u001b[38;5;241m=\u001b[39m rf_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom Forest:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "start_time = time.time()\n",
    "rf_model = RandomForestRegressor(n_estimators=50, max_depth=10, random_state=42)  # Limited depth and estimators\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_preds = rf_model.predict(X_test)\n",
    "print(\"Random Forest:\")\n",
    "print(\"MSE:\", mean_squared_error(y_test, rf_preds))\n",
    "print(\"R2 Score:\", r2_score(y_test, rf_preds))\n",
    "print(f\"Elapsed Time: {time.time() - start_time:.2f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "start_time = time.time()\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=50, max_depth=10, verbosity=1, random_state=42)  # Reduced complexity\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_preds = xgb_model.predict(X_test)\n",
    "print(\"XGBoost:\")\n",
    "print(\"MSE:\", mean_squared_error(y_test, xgb_preds))\n",
    "print(\"R2 Score:\", r2_score(y_test, xgb_preds))\n",
    "print(f\"Elapsed Time: {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, preds)\n",
    "        r2 = r2_score(y_test, preds)\n",
    "        mae = mean_absolute_error(y_test, preds)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        return mse, r2, mae, elapsed_time\n",
    "\n",
    "    # Linear Regression Model Evaluation\n",
    "lr_model = LinearRegression()\n",
    "print(\"Linear Regression Evaluation:\")lr_mse, lr_r2, lr_mae, lr_elapsed_time = evaluate_model(lr_model, X_train, X_test, y_train, y_test)print(f\"MSE: {lr_mse}, R2 Score: {lr_r2}, MAE: {lr_mae}, Elapsed Time: {lr_elapsed_time:.2f} seconds\\n\")\n",
    "\n",
    "    # Random Forest Model Evaluation\n",
    "rf_model = RandomForestRegressor(n_estimators=50, max_depth=10, random_state=42)  # Limited depth and estimators\n",
    "print(\"Random Forest Evaluation:\")\n",
    "rf_mse, rf_r2, rf_mae, rf_elapsed_time = evaluate_model(rf_model, X_train, X_test, y_train, y_test)\n",
    "print(f\"MSE: {rf_mse}, R2 Score: {rf_r2}, MAE: {rf_mae}, Elapsed Time: {rf_elapsed_time:.2f} seconds\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
